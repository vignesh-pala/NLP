{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NL_Keras_Ch8",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjxsI5M2skInYeoRM09zSs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vignesh-pala/NLP/blob/master/NL_Keras_Ch8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttsotvPWnZsd",
        "colab_type": "text"
      },
      "source": [
        "**Challenge 8 (NLP with Keras-TF2)**\n",
        "\n",
        " \n",
        "\n",
        "Implement the following on the built in reuters dataset in keras\n",
        "\n",
        " \n",
        "\n",
        "1. Vectorization and Normalization\n",
        "\n",
        "2. One Hot encoding for labels using the built in to_categorical util\n",
        "\n",
        "3. Use these layers to build the model - Sequential, Dense, Dropout\n",
        "\n",
        "4. Achieve a model accuracy of around 82%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CPtRw1-jr9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import reuters\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import preprocessing\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, BatchNormalization\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras.regularizers import l1, l2, l1_l2\n",
        "from matplotlib import pyplot\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras import models, layers, backend"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGNz8wl3jyn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words = 5000"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrxAXuYpj3wn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a7f5442e-ea7f-4bb7-c55c-ad274c7d7dc4"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.1)\n",
        "#word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "print(x_train[0])"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbmXEb0Fj4s9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "dc614d22-88a8-4f9e-ae5d-a1fd688c70f5"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train_tkn = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test_tkn = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "print(x_train_tkn[0])"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 1. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz2Vl5uUoK-5",
        "colab_type": "text"
      },
      "source": [
        "Convert labels to one-hot encoded fields, so each field represents a label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAiIyxEfkUyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a02a6474-0e1d-46a6-b223-bf0159f74e60"
      },
      "source": [
        "y_train_cat = np_utils.to_categorical(y_train)\n",
        "y_test_cat = np_utils.to_categorical(y_test)\n",
        "\n",
        "categories = len(y_train_cat[0])\n",
        "\n",
        "print('Total categories = {}'.format(categories))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total categories = 46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwbJFFG4n4Eb",
        "colab_type": "text"
      },
      "source": [
        "**Pad Sequences**\n",
        "\n",
        "* Keras prefers inputs to be vectorized and all inputs to have the same length.\n",
        "So we need to fill in the remaining length with '0' to make the vector length uniform across the dataset.\n",
        "* To acheive this, we use the pad_sequences()\n",
        "Note: padding='post' ensures the 0 padding is done at the end of each record"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1uhn_tXmyua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_pad = preprocessing.sequence.pad_sequences(x_train_tkn, maxlen=max_words,  padding='post')\n",
        "x_test_pad = preprocessing.sequence.pad_sequences(x_test_tkn, maxlen=max_words,  padding='post')"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozF3ynlioOMN",
        "colab_type": "text"
      },
      "source": [
        "**Model Building:**\n",
        "* Dropout Layer: Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eRqJdMhlMgl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "8f8eb1d3-5b04-4621-9b15-5029b89f8978"
      },
      "source": [
        "backend.clear_session()\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(256, activation='relu', input_shape=(max_words,)))\n",
        "model.add(layers.Dropout(0.4))\n",
        "model.add(layers.Dense(categories, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, mode='auto', baseline=None, restore_best_weights=False)\n",
        "history = model.fit(x_train_pad, y_train_cat,\n",
        "                    epochs=3,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(x_test_pad, y_test_cat), callbacks=[early_stop])"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 256)               1280256   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 46)                11822     \n",
            "=================================================================\n",
            "Total params: 1,292,078\n",
            "Trainable params: 1,292,078\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10105 samples, validate on 1123 samples\n",
            "Epoch 1/3\n",
            "10105/10105 [==============================] - 3s 328us/step - loss: 1.3341 - accuracy: 0.7105 - val_loss: 0.9993 - val_accuracy: 0.7711\n",
            "Epoch 2/3\n",
            "10105/10105 [==============================] - 3s 314us/step - loss: 0.6991 - accuracy: 0.8431 - val_loss: 0.8391 - val_accuracy: 0.8139\n",
            "Epoch 3/3\n",
            "10105/10105 [==============================] - 3s 312us/step - loss: 0.4698 - accuracy: 0.8947 - val_loss: 0.8387 - val_accuracy: 0.8237\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}