{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NL_Keras_Ch8",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOePCuA9ohvNo9j7+tM+fCc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vignesh-pala/NLP/blob/master/NL_Keras_Ch8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttsotvPWnZsd",
        "colab_type": "text"
      },
      "source": [
        "**Challenge 8 (NLP with Keras-TF2)**\n",
        "\n",
        " \n",
        "\n",
        "Implement the following on the built in reuters dataset in keras\n",
        "\n",
        " \n",
        "\n",
        "1. Vectorization and Normalization\n",
        "\n",
        "2. One Hot encoding for labels using the built in to_categorical util\n",
        "\n",
        "3. Use these layers to build the model - Sequential, Dense, Dropout\n",
        "\n",
        "4. Achieve a model accuracy of around 82%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CPtRw1-jr9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import Tensorflow.Keras libraries\n",
        "import tensorflow \n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from tensorflow.keras.datasets import reuters\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import models,layers,regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import models, layers, backend\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from matplotlib import pyplot\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGNz8wl3jyn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words = 5000"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrxAXuYpj3wn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0cbaa895-7b2e-4f0e-b678-1840e4d6d5ed"
      },
      "source": [
        "# split into Train, Test and Validation set\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.1)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)\n",
        "\n",
        "#word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "print(x_train[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 53, 996, 26, 14, 924, 26, 39, 19, 2, 18, 14, 19, 3302, 18, 86, 187, 63, 11, 14, 160, 59, 11, 17, 12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbmXEb0Fj4s9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "9d2b1794-904d-4f29-ef1c-a77b4d54e0b9"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train_tkn = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test_tkn = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "x_val_tkn = tokenizer.sequences_to_matrix(x_val, mode='binary')\n",
        "print(x_train_tkn[0])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 1. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz2Vl5uUoK-5",
        "colab_type": "text"
      },
      "source": [
        "Convert labels to one-hot encoded fields, so each field represents a label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAiIyxEfkUyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "71e8d70b-1874-4fcb-cad8-dcace3c8f2ca"
      },
      "source": [
        "y_train_cat = np_utils.to_categorical(y_train)\n",
        "y_test_cat = np_utils.to_categorical(y_test)\n",
        "y_val_cat = np_utils.to_categorical(y_val)\n",
        "\n",
        "categories = len(y_train_cat[0])\n",
        "\n",
        "print('Total categories = {}'.format(categories))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total categories = 46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwbJFFG4n4Eb",
        "colab_type": "text"
      },
      "source": [
        "**Pad Sequences**\n",
        "\n",
        "* Keras prefers inputs to be vectorized and all inputs to have the same length.\n",
        "So we need to fill in the remaining length with '0' to make the vector length uniform across the dataset.\n",
        "* To acheive this, we use the pad_sequences()\n",
        "Note: padding='post' ensures the 0 padding is done at the end of each record"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1uhn_tXmyua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_pad = preprocessing.sequence.pad_sequences(x_train_tkn, maxlen=max_words,  padding='post')\n",
        "x_test_pad = preprocessing.sequence.pad_sequences(x_test_tkn, maxlen=max_words,  padding='post')\n",
        "x_val_pad = preprocessing.sequence.pad_sequences(x_val_tkn, maxlen=max_words,  padding='post')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozF3ynlioOMN",
        "colab_type": "text"
      },
      "source": [
        "**Model Building:**\n",
        "* Train the model and validate on the Validation set\n",
        "* Dropout Layer: Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eRqJdMhlMgl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "997a7ca3-be12-436b-e045-d8f5deb78751"
      },
      "source": [
        "backend.clear_session()\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(256, activation='relu', input_shape=(max_words,)))\n",
        "model.add(layers.Dropout(0.4))\n",
        "model.add(layers.Dense(categories, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1, mode='auto', baseline=None, restore_best_weights=False)\n",
        "history = model.fit(x_train_pad, y_train_cat,\n",
        "                    epochs=10,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(x_val_pad, y_val_cat), callbacks=[early_stop])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 256)               1280256   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 46)                11822     \n",
            "=================================================================\n",
            "Total params: 1,292,078\n",
            "Trainable params: 1,292,078\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 9094 samples, validate on 1011 samples\n",
            "Epoch 1/10\n",
            "9094/9094 [==============================] - 3s 297us/step - loss: 1.3952 - accuracy: 0.7009 - val_loss: 0.9853 - val_accuracy: 0.7883\n",
            "Epoch 2/10\n",
            "9094/9094 [==============================] - 3s 295us/step - loss: 0.7284 - accuracy: 0.8419 - val_loss: 0.8052 - val_accuracy: 0.8200\n",
            "Epoch 3/10\n",
            "9094/9094 [==============================] - 3s 294us/step - loss: 0.4854 - accuracy: 0.8927 - val_loss: 0.7918 - val_accuracy: 0.8299\n",
            "Epoch 4/10\n",
            "9094/9094 [==============================] - 3s 297us/step - loss: 0.3627 - accuracy: 0.9179 - val_loss: 0.8049 - val_accuracy: 0.8318\n",
            "Epoch 5/10\n",
            "9094/9094 [==============================] - 3s 303us/step - loss: 0.2952 - accuracy: 0.9322 - val_loss: 0.8723 - val_accuracy: 0.8269\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mfr-LlbddpO",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the model on Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olbre36GdZRj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "c4431592-226a-4054-8c6f-79bda74916a5"
      },
      "source": [
        "results = model.evaluate(x_test_pad, y_test_cat, batch_size=64)\n",
        "print(\"test loss = {}\\ntest accuracy = {}\".format(results[0], results[1]))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1123/1123 [==============================] - 0s 83us/step\n",
            "test loss = 0.9978815156863613\n",
            "test accuracy = 0.8076580762863159\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}